<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Transformers y modelos de atención | Juan Antonio Pérez-Ortiz</title> <meta name="author" content="Juan Antonio Pérez-Ortiz"> <meta name="description" content="Conociendo los modelos de atención y su aplicación en el modelo transformer"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/me2/assets/css/main.css"> <link rel="canonical" href="https://jaspock.github.io/me2/materials/transformers/attention"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/me2/assets/js/theme.js"></script> <script src="/me2/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/me2/"><span class="font-weight-bold">Juan Antonio </span>Pérez-Ortiz</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/me2/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/materials/">materials</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Transformers y modelos de atención</h1> <p class="post-description">Conociendo los modelos de atención y su aplicación en el modelo transformer</p> </header> <article> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/me2/assets/img/transformers/engine-words-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/me2/assets/img/transformers/engine-words-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/me2/assets/img/transformers/engine-words-1400.webp"></source> <img src="/me2/assets/img/transformers/engine-words.png" class="img-fluid rounded z-depth-1" width="256px" height="256px" title="ai-generated image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p class="small-text">Nota: este capítulo es parte de la serie “<a href="../transformers">Un recorrido peso a peso por el transformer</a>”, donde se presenta una guía para aprender cómo funcionan las redes neuronales que procesan textos y cómo se programan.</p> <h2 id="fundamentos-de-los-transformers">Fundamentos de los transformers</h2> <p><i class="fas fa-clock"></i> 6 horas</p> <p>Ahora sí podemos acometer el estudio de los elementos básicos de la arquitectura transformer siguiendo el capítulo “<a href="https://web.archive.org/web/20221218211150/https://web.stanford.edu/~jurafsky/slp3/9.pdf" rel="external nofollow noopener" target="_blank">Deep Learning Architectures for Sequence Processing</a>” <a href="https://web.archive.org/web/20221218211150/https://web.stanford.edu/~jurafsky/slp3/9.pdf" rel="external nofollow noopener" target="_blank"><i class="fas fa-file"></i></a>. Aquí entenderás qué significa una de las ecuaciones más importantes de los últimos años dentro del aprendizaje automático <d-cite key="vaswani-attention-2017"></d-cite>:</p> \[\text{Atención}(Q,K,V) = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) \, V\] <p>Salta las secciones 9.2 a 9.6, que se centran en otro modelo alternativo para el procesamiento de secuencias, las redes neuronales recurrentes, que se han venido usando menos en el área del procesamiento del lenguaje natural tras la llegada del transformer.</p> <h2 id="anotaciones-al-libro">Anotaciones al libro</h2> <p>Es recomendable que estudies estos comentarios después de una primera lectura del capítulo y antes de la segunda lectura.</p> <h4 id="apartado">Apartado</h4> <h2 id="atención-escalada">Atención escalada</h2> <p>Un factor que puede parecer arbitrario en la ecuación de la atención es la división por la raíz cuadrada de la dimensión de la clave. Para entender la motivación de esta operación, observa que cuanto mayor es el tamaño de los embeddings, mayor es el resultado de cada producto escalar \(q_i k_j\). El problema es que cuando la función softmax se aplica a valores muy altos, su carácter exponencial hace que asigne valores muy pequeños a todos los elementos excepto al que tiene el valor más alto. Es decir, cuando la función softmax se satura, tiende a un vector <em>one-hot</em>. Esto provocará que la atención se centre en un único token e ignore el resto, lo que no es un comportamiento deseable.</p> <p>Consideremos que \(Q\) y \(K\) tienen tamaño \(B \times T \times C\), donde \(C\) es el tamaño de las consultas y las claves. Para simplificar, si asumimos que los elementos de las matrices \(Q\) y \(K\) tienen varianza alrededor de 1, la varianza de los elementos del producto será del orden de \(C\). Como se cumple que, dado un escalar \(m\), \(\mathrm{var}(mX) = m^2 \mathrm{var}(x)\), al multiplicar cada elemento por \(1 / \sqrt{C}\), la varianza del producto matricial se reduce en \(\left(1 / \sqrt{C}\right)^2 = 1 / C\). Por tanto, si la varianza de los elementos de \(Q\) y \(K\) es 1, ahora la varianza del producto matricial también estará alrededor de 1.</p> <p>El siguiente código permite comprobar los extremos anteriores:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5000</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span><span class="o">*</span><span class="n">m</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span><span class="o">*</span><span class="n">m</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Variance of Q: </span><span class="si">{</span><span class="n">Q</span><span class="p">.</span><span class="nf">var</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Variance of K: </span><span class="si">{</span><span class="n">K</span><span class="p">.</span><span class="nf">var</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="c1"># variances close to m**2
</span>
<span class="n">QK</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Variance of QK: </span><span class="si">{</span><span class="n">QK</span><span class="p">.</span><span class="nf">var</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span> 
<span class="c1"># very high variance close to C*(m**4)!
</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">QK</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Mean value of highest softmax: </span><span class="si">{</span><span class="n">s</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span> 
<span class="c1"># max value of each channel close to 1
</span>
<span class="n">QK</span> <span class="o">=</span> <span class="n">QK</span> <span class="o">/</span> <span class="p">(</span><span class="n">C</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Variance of QK after normalization: </span><span class="si">{</span><span class="n">QK</span><span class="p">.</span><span class="nf">var</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="c1"># variance close to m**4
</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">QK</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Mean value of highest softmax: </span><span class="si">{</span><span class="n">s</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span> 
<span class="c1"># max value of each channel smaller than 1
</span></code></pre></div></div> <p>En general, si la varianza de los elementos de \(Q\) y \(K\) es \(m\), la varianza del producto matricial estará alrededor de \(m^4 C\). Si \(m=2\), por ejemplo, la normalización no nos deja elementos con varianza de 1, pero sí la reduce para dejarla en el orden de \(m^4 = 16\).</p> <h2 id="la-arquitectura-transformer-completa">La arquitectura transformer completa</h2> <p><i class="fas fa-clock"></i> 2 horas</p> <p>En el bloque anterior, hemos estudiado todos los elementos del transformer, pero en aplicaciones en las que una secuencia de tokens se transforma en otra secuencia de tokens, las capas del transformen se asocian a dos submodelos bien diferenciados y conectados entre ellos mediante mecanismos de atención: el codificador y el descodificador. El capítulo “<a href="https://web.archive.org/web/20221218211150/https://web.stanford.edu/~jurafsky/slp3/10.pdf" rel="external nofollow noopener" target="_blank">Machine translation</a>” <a href="https://web.archive.org/web/20221218211150/https://web.stanford.edu/~jurafsky/slp3/10.pdf" rel="external nofollow noopener" target="_blank"><i class="fas fa-file"></i></a> se centra en la arquitectura completa. En este capítulo solo es necesario que estudies las secciones 10.2 y 10.6.</p> <h2 id="aspectos-adicionales-sobre-el-uso-de-transformers-en-procesamiento-del-lenguaje-natural">Aspectos adicionales sobre el uso de transformers en procesamiento del lenguaje natural</h2> <p>Hay algunos elementos adicionales a la definición de los diferentes modelos neuronales que son también relevantes cuando estos se aplican en el área del procesamiento del lenguaje natural.</p> <ul> <li>Estudia el mecanismo de búsqueda en haz (<em>beam search</em>) descrito en la sección 10.5.</li> <li>Lee lo que se dice sobre la obtención de subpalabras en las secciones 10.7.1 y 2.4.3.</li> </ul> <h2 id="modelos-preentrenados">Modelos preentrenados</h2> <p><i class="fas fa-clock"></i> 2 horas</p> <p>El capítulo “<a href="https://web.archive.org/web/20221218211150/https://web.stanford.edu/~jurafsky/slp3/11.pdf" rel="external nofollow noopener" target="_blank">Transfer Learning with Contextual Embeddings and Pre-trained language models</a>” <a href="https://web.archive.org/web/20221218211150/https://web.stanford.edu/~jurafsky/slp3/11.pdf" rel="external nofollow noopener" target="_blank"><i class="fas fa-file"></i></a> estudia los modelos preentrenados y cómo adaptarlos a nuestras necesidades. Este material puede ser opcional o incluso innecesario para ti; consulta con el profesor. En el caso de que abordes su estudio, pueden ser relevantes la introducción y las secciones 11.1 y 11.2.</p> <h2 id="implementación-en-pytorch">Implementación en PyTorch</h2> <p>Estudiamos aquí dos arquitecturas muy similares:</p> <ul> <li>Una implementación del <a href="https://github.com/jaspock/me/blob/master/assets/code/transformers/transformer.py" rel="external nofollow noopener" target="_blank">transformer</a> <a href="https://github.com/jaspock/me/blob/master/assets/code/transformers/transformer.py" rel="external nofollow noopener" target="_blank"><i class="fas fa-file"></i></a>. La implementación sigue la del artículo “<a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Attention is all you need</a>” de 2017, que puedes consultar si necesitas más información.</li> <li>Una implementación sencilla del <a href="https://github.com/jaspock/me/blob/master/assets/code/transformers/bert.py" rel="external nofollow noopener" target="_blank">modelo BERT</a> <a href="https://github.com/jaspock/me/blob/master/assets/code/transformers/bert.py" rel="external nofollow noopener" target="_blank"><i class="fas fa-file"></i></a>.</li> </ul> <p>A estas alturas, ya deberías estar dedicando tiempo a estudiar las implementaciones de los modelos, pero recuerda empezar por las más sencillas de bloques anteriores.</p> <h2 id="ejercicios-de-repaso">Ejercicios de repaso</h2> <ol> <li> <p>Argumenta la verdad o falsedad de la siguiente afirmación: si la consulta de un token \(q_i\) es igual a su clave \(k_i\), entonces el embedding computado por el mecanismo de autoatención para dicho token coincide con su valor \(v_i\).</p> </li> <li> <p>La siguiente fórmula para el cálculo de la auto-atención en el descodificador de un transformer es ligeramente distinta a la habitual, ya que se ha añadido explícitamente la máscara que impide que, como hemos visto, la atención calculada para un token durante el entrenamiento tenga en cuenta los tokens que se encuentran posteriormente en la frase:</p> \[\text{Atención}(Q,K,V,M) = \text{softmax} \left( \frac{Q K^T + M}{\sqrt{d_k}} \right) \, V\] <p>Indica qué forma tiene la matriz de máscara \(M\). Busca las operaciones de PyTorch que te permiten inicializar dicha matriz. Si necesitas usar valores de infinito en el código, razona si es suficiente con utilizar un número grande como \(10^9\).</p> </li> <li> <p>Razona cómo se usaría una máscara \(M\) similar a la de la fórmula anterior para enmascarar los tokens de relleno de un mini-batch de frases de entrenamiento.</p> </li> <li> <p>Dada una secuencia de tokens de entrada, el codificador (<em>encoder</em>) de un transformer permite obtener un conjunto de embeddings para cada token de entrada. Estos embeddings son contextuales en todas sus capas menos en una. ¿En qué capa los embeddings no son contextuales? ¿Cómo se obtienen los valores finales de estos embeddings? ¿Y los valores iniciales?</p> </li> <li> <p>Supongamos que el embedding no contextual almacenado en la tabla de embeddings del codificador de un transformer para un determinado token viene definido por el vector \(\mathbf{e} = \mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n\). Consideremos el caso en el que dicho token aparece en dos posiciones diferentes de una frase de entrada. Si no se utilizaran embeddings posicionales, ¿cuál sería el valor del coseno entre los vectores de los dos embeddings no contextuales usados para el token? Llamemos ahora \(\mathbf{p}\) y \(\mathbf{p}’\) a los embeddings posicionales que se utilizan en cada una de las dos apariciones del token. El coseno del ángulo entre los vectores resultantes de sumar al embedding e los embeddings posicionales sería:</p> \[\cos(\alpha)=\frac{\sum_{i=1}^d(\mathbf{e}_i+\mathbf{p}_i)(\mathbf{e}_i+\mathbf{p}'_i)}{\sqrt{\left(\sum_{j=1}^d(\mathbf{e}_j+\mathbf{p}_j)^2\right)\left(\sum_{j=1}^d(\mathbf{e}_j+\mathbf{p}_j')^2\right)}}\] <p>¿A qué valor se irá acercando el coseno anterior cuando la distancia que separa las dos apariciones del token en la frase vaya aumentando? Razona tu respuesta.</p> </li> <li> <p>Una matriz de atención representa en cada fila el nivel de atención que se presta a los embeddings calculados para cada token de la frase en la capa anterior (representados en las columnas) cuando se va a calcular el embedding del token de dicha fila en un determinado cabezal (head) del codificador de un transformer. Un color más oscuro representa mayor atención. Considera ahora un cabezal (head) de un codificador de un transformer que presta una atención aproximadamente monótona sobre los embeddings de la capa anterior. En particular, para cada token se atiende en un elevado grado al embedding de ese mismo token en la capa anterior y con mucha menor intensidad al token inmediatamente a su derecha; el resto de embeddings de la capa anterior no reciben atención. Dibuja aproximadamente la matriz de atención resultante sobre la misma frase que la de la figura.</p> </li> </ol> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Juan Antonio Pérez-Ortiz. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/me2/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/me2/assets/js/zoom.js"></script> <script defer src="/me2/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>