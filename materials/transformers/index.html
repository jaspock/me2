<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Un recorrido peso a peso por el transformer | Juan Antonio Pérez-Ortiz</title> <meta name="author" content="Juan Antonio Pérez-Ortiz"> <meta name="description" content="Guía para aprender cómo funcionan las redes neuronales que procesan textos y cómo se programan"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/me2/assets/css/main.css"> <link rel="canonical" href="https://jaspock.github.io/me2/materials/transformers/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/me2/assets/js/theme.js"></script> <script src="/me2/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/me2/"><span class="font-weight-bold">Juan Antonio </span>Pérez-Ortiz</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/me2/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/materials/">materials</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/me2/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Un recorrido peso a peso por el transformer</h1> <p class="post-description">Guía para aprender cómo funcionan las redes neuronales que procesan textos y cómo se programan</p> </header> <article> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/me2/assets/img/transformers/math-mandelbrot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/me2/assets/img/transformers/math-mandelbrot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/me2/assets/img/transformers/math-mandelbrot-1400.webp"></source> <img src="/me2/assets/img/transformers/math-mandelbrot.png" class="img-fluid rounded z-depth-1" width="256px" height="256px" title="ai-generated image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="contenidos">Contenidos</h2> <ul> <li><a href="#introducci%C3%B3n">Introducción</a></li> <li><a href="regresor.html">Regresión logística</a></li> <li><a href="embeddings.html">Embbedings incontextuales</a></li> <li><a href="ffw.html">Redes hacia delante</a></li> <li> <p><a href="attention.html">Transformers y modelos de atención</a></p> </li> <li><a href="pytorch.html">Aprender a programar con PyTorch</a></li> <li><a href="apuntes.html">Apuntes de PyTorch</a></li> </ul> <h2 id="introducción">Introducción</h2> <p>Esta guía propone una camino para entender cómo funciona realmente la red neuronal más usada en el campo del procesamiento del lenguaje natural (conocida como <em>transformer</em>). Se siguen para ello las explicaciones teóricas de algunos de los capítulos de un buen libro sobre la materia. Se propone ir aprendiendo sobre la marcha (si es necesario) el lenguaje de programación Python, así como los elementos básicos de una librería llamada PyTorch que permite, entre otras cosas, programar redes neuronales que se entrenen y ejecuten sobre GPUs. Como colofón, se estudia una implementación ya existente del transformer programada con PyTorch. El objetivo último es poder modificar este código para experimentar con algún problema sencillo que implique el uso del lenguaje humano. La idea es obtener un buen conocimiento que permita afrontar tareas más complejas más adelante y no tanto desarrollar algo llamativo que enseñar a todo el mundo desde el primer momento.</p> <p>Algunos aspectos los puedes ir estudiando en paralelo. A la vez que aprendes sobre modelos neuronales, puedes ir iniciándote en <a href="pytorch#python">Python</a>, NumPy e incluso, una vez vistos los dos anteriores, <a href="pytorch#pytorch">PyTorch</a>. También puedes repasar en paralelo los <a href="#conceptos-previos">elementos de álgebra, cálculo y probabilidad</a> que hayas olvidado. El estudio del código del transformer no deberías abordarlo hasta tener bien asimilados todos los conceptos anteriores.</p> <p>Para entender a nivel matemático y conceptual las redes neuronales nos vamos a basar en la tercera edición (todavía inacabada) del libro “<a href="https://web.archive.org/web/20221218211150/https://web.stanford.edu/~jurafsky/slp3/" rel="external nofollow noopener" target="_blank">Speech and Language Processing</a>” de Dan Jurafsky y James H. Martin. Los apartados de esta guía van indicando qué capítulos y secciones son relevantes para nuestros propósitos. <strong>Importante</strong>: dado que la versión en línea del libro está inacabada y se actualiza de vez en cuando, no solo con nuevos contenidos, sino también con reestructuraciones de los ya existentes y movimientos de secciones de un capítulo a otro, en esta guía se incluyen enlaces y referencias a una <a href="https://web.archive.org/web/20221218211150/https://web.stanford.edu/~jurafsky/slp3/" rel="external nofollow noopener" target="_blank">versión del libro alojada en Internet Archive</a> que probablemente no se corresponde con la más actual (que puedes encontrar <a href="https://web.stanford.edu/~jurafsky/slp3/" rel="external nofollow noopener" target="_blank">aquí</a>).</p> <h2 id="justificación-de-un-abordaje-profundo">Justificación de un abordaje profundo</h2> <p>En principio, escribir un programa que explote modelos basados en aprendizaje automático es muy sencillo. Por ejemplo, las siguientes líneas de código usan un modelo de lengua basado en transformer para continuar un texto dado:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="n">generator</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="s">'text-generation'</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="s">'gpt2'</span><span class="p">)</span>
<span class="nf">generator</span><span class="p">(</span><span class="s">"Hello, I'm a language model and"</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span></code></pre></figure> <p>Aunque la existencia de librerías de alto nivel es sumamente importante en ciertos contextos, si solo usas el código anterior:</p> <ul> <li>No entenderás cómo funciona realmente el modelo.</li> <li>No podrás crear otros modelos para experimentar con otros problemas.</li> <li>No sabrás cómo entrenar un modelo propio ni qué elementos influyen en la calidad o el tiempo del entrenamiento.</li> <li>No entenderás otros modelos neuronales que se usan en el procesamiento del lenguaje natural.</li> <li>Verás tu programa, en definitiva, como una caja negra que hace cosas mágicas.</li> </ul> <p>Esta guía pretende ayudarte a abrir la caja y ser capaz de observar su interior con conocimiento de causa.</p> <h2 id="conceptos-matemáticos-previos">Conceptos matemáticos previos</h2> <p>Los elementos básicos de álgebra, cálculo y probabilidad necesarios para manejarte con soltura en el mundo del procesamiento del lenguaje natural los puedes encontrar en las secciones “Linear Algebra”, “Calculus” (junto con “Automatic differentiation”) y “Probability and Statistics” del <a href="https://d2l.ai/chapter_preliminaries/index.html" rel="external nofollow noopener" target="_blank">capítulo 2</a> <a href="cap2"><i class="fas fa-file"></i></a> del libro “Dive into Deep Learning”. Otros como la teoría de la información (19.11) o el principio de máxima verosimilitud (19.7) se abordan en un <a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html" rel="external nofollow noopener" target="_blank">apéndice</a> <a href="ap%C3%A9ndice"><i class="fas fa-file"></i></a> del mismo libro.</p> <h2 id="para-saber-más">Para saber más</h2> <p>Para ampliar conocimientos, puedes consultar estos libros que, en su mayoría, están disponibles en la web:</p> <ul> <li>“<a href="https://web.stanford.edu/~jurafsky/slp3/" rel="external nofollow noopener" target="_blank">Speech and Language Processing</a>” de Dan Jurafsky y James H. Martin. Sin fecha de publicación aún, pero con un borrador avanzado disponible en la web. Explica detalladamente los conceptos y modelos más relevantes en el procesamiento del lenguaje natural sin entrar en detalles de implementación. Es el libro en el que se basa esta guía.</li> <li>“<a href="https://leanpub.com/pytorch" rel="external nofollow noopener" target="_blank">Deep Learning with PyTorch Step-by-Step: A Beginner’s Guide</a>” (2022) de Daniel Voigt Godoy. Este es un libro de pago con versión digital o en papel (en tres volúmenes, en este caso). Existe una versión en español para los primeros capítulos. Escrito en un estilo directo y sencillo con multitud de detalles y ejemplos.</li> <li>“<a href="http://d2l.ai/" rel="external nofollow noopener" target="_blank">Dive into Deep Learning</a>” de Aston Zhang, Zachary C. Lipton, Mu Li y Alexander J. Smola. Se adentra con mucho detalle en la implementación de los modelos más relevantes del aprendizaje profundo. Hay una versión en <a href="https://www.cambridge.org/es/academic/subjects/computer-science/pattern-recognition-and-machine-learning/dive-deep-learning?format=PB" rel="external nofollow noopener" target="_blank">papel</a> publicada en 2023.</li> <li>“<a href="https://udlbook.github.io/udlbook/" rel="external nofollow noopener" target="_blank">Understanding Deep Learning</a>” por Simon J.D. Prince. A publicar en 2023, pero con un borrador ya disponible. Lleno de imágenes y figuras que ayudan a entender todos los conceptos.</li> <li>La serie “<a href="https://probml.github.io/pml-book/book1.html" rel="external nofollow noopener" target="_blank">Probabilistic Machine Learning: An Introduction</a>” (2022) y “<a href="https://probml.github.io/pml-book/book2.html" rel="external nofollow noopener" target="_blank">Probabilistic Machine Learning: Advanced Topics</a>” (2023) de Kevin Murphy aborda con más profundidad diversos elementos del aprendizaje automático.</li> <li>“<a href="https://clulab.org/gentlenlp.html" rel="external nofollow noopener" target="_blank">Deep learning for Natural Language Processing: A Gentle Introduction</a>” de Mihai Surdeanu y Marco A. Valenzuela-Escárcega. También en elaboración. Contiene código en algunos capítulos.</li> </ul> <p>La siguiente lista contiene enlaces a algunos cursos en vídeo impartidos por investigadores o universidades reconocidos:</p> <ul> <li>“<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ" rel="external nofollow noopener" target="_blank">Stanford CS224n</a> - Natural Language Processing with Deep Learning” por Christopher Manning; <a href="http://web.stan%20%20ford.edu/class/cs224n/" rel="external nofollow noopener" target="_blank">web</a> del curso.</li> <li>“<a href="https://stanford-cs324.github.io/winter2022/" rel="external nofollow noopener" target="_blank">Stanforf CS324</a> - Large Language Models”.</li> <li>“<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM" rel="external nofollow noopener" target="_blank">Stanford CS25</a> - Transformers United”; <a href="https://web.stanford.edu/class/cs25/" rel="external nofollow noopener" target="_blank">web</a> del curso.</li> <li>“<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU" rel="external nofollow noopener" target="_blank">Stanford CS229</a> - Machine Learning” de Andrew Ng; <a href="https://cs229.stanford.edu/" rel="external nofollow noopener" target="_blank">web</a> del curso.</li> <li>“<a href="https://youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb" rel="external nofollow noopener" target="_blank">Stanford CS230</a> - Deep Learning” de Andrew Ng; <a href="https://cs230.stanford.edu/" rel="external nofollow noopener" target="_blank">web</a> del curso.</li> <li>“<a href="https://youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI" rel="external nofollow noopener" target="_blank">MIT 6.S191</a> - Introduction to Deep Learning” de Alexander Amini and Ava Soleimany; <a href="http://introtodeeplearning.com" rel="external nofollow noopener" target="_blank">web</a> del curso.</li> <li>“Neural Networks: <a href="https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" rel="external nofollow noopener" target="_blank">Zero to Hero</a>” de Andrew Karpathy.</li> <li>“<a href="https://www.youtube.com/playlist?list=PLxfEOJXRm7eZKJyovNH-lE3ooXTsOCvfC" rel="external nofollow noopener" target="_blank">Machine Learning Specialization</a>” de Andrew Ng.</li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Juan Antonio Pérez-Ortiz. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/me2/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/me2/assets/js/zoom.js"></script> <script defer src="/me2/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>